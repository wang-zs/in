---
title: "最小二乘法"
description: "基础"
layout: post
date: 2016-06-17 03:23:44 +0800
categories: [线性回归]
tags: [jekyll, github]
comments: yes
---
数据拟合中，为什么要让模型的预测数据与实际数据之差的平方而不是绝对值和最小来优化模型参数？

简单说绝对值的和无法转化为一个可解的寻优问题，既然无法寻优如何得到恰当的参数估计呢

首先，我们来考虑数据拟合的实际状况：当我们寻找模型来拟合数据时，偏差是不可避免的存在的。对一个拟合良好的模型而言，这个偏差整体应该是符合正态分布的，这里可能你会问为什么是正态分布？其实这就是个假设，你用什么分布就要在后续的计算中运用相应分布的概率密度函数，而偏差这种东西符合什么分布最靠谱呢？如果你喜欢扔硬币的话就知道硬币一面出现的概率就是0.5，你扔多次某一面出现的次数的概率就是个二项分布，这是离散的，你扔硬币的次数趋向正无穷再来看这个分布就是正态分布。这之间的证明过程涉及斯特林公式神马的，其实这个推导是在一定条件下完成的，想了解的自行放狗。如果你认同这种0.5概率的扔硬币，那么可以假想理想的偏差也是跟硬币某一面出现的概率分布差不多就行了，至于再深入考虑为什么，那就基本是形而上学的东西了，自便。

现在，我们已经知道偏差符合正态分布，那么下一步就是理解另外一种函数——极大似然函数。在模型拟合中，极大似然函数的本质就是让我们用来拟合数据的模型与每一个数据点的更为相符，这就要求偏差的大小应该是基本一致，或者说符合正态分布，那么偏差大小基本一致与不一致怎么区别呢？这里我们用偏差出现的概率相乘的大小来表示。因为概率大小都在0到1之间并符合期望为x的正态分布，两个偏差值越接近中心期望x，乘积越大。极大似然函数就是用来表示这一关系的，当然在这里联乘的形式可以取对数改为概率求和，如果你还有印象的话，正态分布的概率密度函数是欧拉数的幂函数形式，而幂中有一个负号有一个平方，平方就是偏差的平方，负号则将原来求最大值变成了求最小值，这时候反过来看这个极大似然函数的求解其实就是最小二乘法。

本质上来说，模型拟合都可以用极大似然函数求最值来表示，如果你能想办法把你想键的模型转为一个寻优问题，那就可以通过求导等数学方法来解决了，但千万要注意：并不是所有的模型都可以有最优解，有些只有局部最优，有些则压根找不到，需要足够聪明的人转为对偶的凸函数或其他可解的问题才能寻优，此外，数学上证明了的NP-hard问题就别尝试了，更不要尝试NPC问题了。

好了，现在我大概说明白了为什么用平方和了，本质上就是正态分布的概率密度函数所致，那么为什么不是绝对值的和呢？简单说绝对值的和无法转化为一个可解的寻优问题，既然无法寻优如何得到恰当的参数估计呢？就这么简单。

关于
